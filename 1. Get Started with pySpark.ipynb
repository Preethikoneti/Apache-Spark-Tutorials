{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pyspark\n",
    "1. Basic pyspark package\n",
    "2. conda install -c conda-forge pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark Context (sc)\n",
    "1. Spark Context is the heart of any spark application.\n",
    "2. It setups internal services and establishes connection to Spark execution Env.\n",
    "3. Various parameters which can be used with spark are Master, appName, sparkHome, pyFiles, Environment, batchSize, Serializer, conf, Gateway, JSC, Profiler_cls\n",
    "\n",
    "### Life Cycle of Spark Program\n",
    "1. Create RDDs - from external data source or parallize a collection in driver program\n",
    "2. Lazy Transformation - transforms base RDD into new RDD\n",
    "3. Cache RDDs - for future use\n",
    "4. Performs Actions - to execute parallel computations and produce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs\n",
    "1. RDD stands for Resilient Distributed Dataset\n",
    "2. RDDs are building blocks of every spark application and is immutable\n",
    "3. Resilient means fault tolerance and cabable of rebuilding data on failure\n",
    "4. Data is distributed among various nodes in a cluster.\n",
    "5. Collection of partitioned data with primitive values or values of value.\n",
    "6. To work on RDD we have to create new RDDs using transformation and actions\n",
    "\n",
    "### Transformations\n",
    "1. map, flatmap\n",
    "2. filter\n",
    "3. distinct\n",
    "4. reduceByKey\n",
    "5. mapPartitions\n",
    "6. sortBy\n",
    "\n",
    "### Actions\n",
    "1. collect, collectAsMap\n",
    "2. reduce\n",
    "3. countByKey, countByValue\n",
    "4. take\n",
    "5. first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = 4*count/num_samples\n",
    "print(pi)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark  provides the configuration to run a spark application on a local system or a cluster.\n",
    "1. SparkConf object is used to set different parameters which takes priority over the system properties.\n",
    "2. once sparkconf object is passed, it becomes immutable\n",
    "3. \n",
    "```\n",
    "    class SparkConf:\n",
    "        loadDefaults = true\n",
    "        _jvm = None\n",
    "        _jconf = None\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
