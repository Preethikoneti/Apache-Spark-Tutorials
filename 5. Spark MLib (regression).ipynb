{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"mlib-reg\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('mlib-reg').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataFrames - ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "|      0|   1|   2|  3|    4|    5|   6|     7|  8|    9|  10|    11|  12|  13|\n",
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "|0.00632|18.0|2.31|0.0|0.538|6.575|65.2|  4.09|1.0|296.0|15.3| 396.9|4.98|24.0|\n",
      "|0.02731| 0.0|7.07|0.0|0.469|6.421|78.9|4.9671|2.0|242.0|17.8| 396.9|9.14|21.6|\n",
      "|0.02729| 0.0|7.07|0.0|0.469|7.185|61.1|4.9671|2.0|242.0|17.8|392.83|4.03|34.7|\n",
      "|0.03237| 0.0|2.18|0.0|0.458|6.998|45.8|6.0622|3.0|222.0|18.7|394.63|2.94|33.4|\n",
      "|0.06905| 0.0|2.18|0.0|0.458|7.147|54.2|6.0622|3.0|222.0|18.7| 396.9|5.33|36.2|\n",
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"Resources/boston.csv\",inferSchema=True,header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|  13|\n",
      "+--------------------+----+\n",
      "|[0.00632,18.0,2.3...|24.0|\n",
      "|[0.02731,0.0,7.07...|21.6|\n",
      "|[0.02729,0.0,7.07...|34.7|\n",
      "|[0.03237,0.0,2.18...|33.4|\n",
      "|[0.06905,0.0,2.18...|36.2|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=['0','1','2','3','4','5','6','7','8','9','10','11','12'],\n",
    "                                  outputCol='features')\n",
    "v_boston = vectorAssembler.transform(df)\n",
    "v_boston = v_boston.select(['features','13'])\n",
    "v_boston.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_df,test_df) = v_boston.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol='features',labelCol='13')\n",
    "lr_model = lr.fit(train_df)\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.334699364240313\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE: \" + str(training_summary.meanAbsoluteError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features|  13|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[0.01301,35.0,1.5...|32.7|29.995315349159245|\n",
      "|[0.01381,80.0,0.4...|50.0| 41.09489863381874|\n",
      "|[0.01432,100.0,1....|31.6| 32.92170894032721|\n",
      "|[0.02055,85.0,0.7...|24.7|24.590997592138343|\n",
      "|[0.02729,0.0,7.07...|34.7|30.683213879398213|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1919906014237633"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"13\",metricName=\"mae\")\n",
    "lr_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RDDs- mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "|      0|   1|   2|  3|    4|    5|   6|     7|  8|    9|  10|    11|  12|  13|\n",
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "|0.00632|18.0|2.31|0.0|0.538|6.575|65.2|  4.09|1.0|296.0|15.3| 396.9|4.98|24.0|\n",
      "|0.02731| 0.0|7.07|0.0|0.469|6.421|78.9|4.9671|2.0|242.0|17.8| 396.9|9.14|21.6|\n",
      "|0.02729| 0.0|7.07|0.0|0.469|7.185|61.1|4.9671|2.0|242.0|17.8|392.83|4.03|34.7|\n",
      "|0.03237| 0.0|2.18|0.0|0.458|6.998|45.8|6.0622|3.0|222.0|18.7|394.63|2.94|33.4|\n",
      "|0.06905| 0.0|2.18|0.0|0.458|7.147|54.2|6.0622|3.0|222.0|18.7| 396.9|5.33|36.2|\n",
      "+-------+----+----+---+-----+-----+----+------+---+-----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"Resources/boston.csv\",inferSchema=True,header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "df_pd.replace('?',np.NaN,inplace=True)\n",
    "df_pd.dropna(inplace=True)\n",
    "df_pd.to_csv(\"Resources/boston_new.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "points = sc.textFile(\"Resources/boston_new.csv\")\n",
    "def prepareLabelledPoints(row):\n",
    "    values = [float(s) for s in row.strip().split(',')]\n",
    "    return LabeledPoint(float(values[13]),values[:13])\n",
    "pointrdd = points.map(prepareLabelledPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(24.0, [0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]),\n",
       " LabeledPoint(21.6, [0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14]),\n",
       " LabeledPoint(34.7, [0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03]),\n",
       " LabeledPoint(33.4, [0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622,3.0,222.0,18.7,394.63,2.94]),\n",
       " LabeledPoint(36.2, [0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622,3.0,222.0,18.7,396.9,5.33])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointrdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainData,testData) = pointrdd.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "trained_model = LinearRegressionWithSGD.train(trainData,100,step=0.01,initialWeights=np.random.rand(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights : [2.06740410926e+269,4.3468025625e+269,5.43075930541e+269,3.08127200446e+267,2.53470152037e+268,2.77940879411e+269,3.21135468607e+270,1.59404065165e+269,5.02518404161e+269,1.98949404805e+271,8.35987817719e+269,1.60940010232e+271,6.01260635242e+269]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights : {}\".format(trained_model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2513852241057594e+274,\n",
       " 1.1483475144266899e+274,\n",
       " 1.0937954264995798e+274,\n",
       " 1.2876862920940184e+274,\n",
       " 1.2687858031293608e+274]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trained_model.predict(testData.map(lambda x: x.features))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 1.3733044483531845e+274\n"
     ]
    }
   ],
   "source": [
    "pred = predictions.collect()\n",
    "labels = testData.map(lambda x: x.label).collect()\n",
    "total_samples = len(pred)\n",
    "mae = 0\n",
    "for p,l in zip(pred,labels):\n",
    "    mae += abs(p-l)\n",
    "mae/=total_samples\n",
    "print(\"Mean absolute error: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
